---
title: "Gaussian Data Example"
author: "Eric Dunipace"
date: "5/6/2019"
output:
  html_document: default
  pdf_document: default
---
<style>
p.caption {
  font-size: 0.9em;
  font-style: italic;
  color: grey;
  margin-right: 10%;
  margin-left: 10%;  
  text-align: justify;
}
</style>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(SparsePosterior)
require(ggplot2)
require(CoarsePosteriorSummary)
require(rstan)
require(ggsci)
require(doParallel)
require(ggridges)
require(data.table)
```
```{r data, message= FALSE, warning = FALSE, echo = FALSE}
#### Load packages ####
require(SparsePosterior)
require(ggplot2)
require(CoarsePosteriorSummary)
require(rstan)
require(ggsci)
require(doParallel)
require(ggridges)
require(data.table)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores()-1)

group.names <- c("Selection","Loc./Scale", "Projection")

#### generate data ####
set.seed(9507123)
n <- 1024
p <- 31
nsamp <- 1000
nlambda <- 100
lambda.min.ratio <- 1e-10
gamma <- 1
pseudo.observations <- 0

target <- get_normal_linear_model()
param <- target$rparam()
p_star <- length(param$theta)

target$X$corr <- 0.5
X <- target$X$rX(n, target$X$corr, p)
Y <- target$rdata(n,X[,1:p_star], param$theta, param$sigma2)

x <- target$X$rX(1, target$X$corr, p)

true_mu_full <- X[,1:p_star, drop=FALSE] %*% c(param$theta)
true_mu <- x[,1:p_star,drop=FALSE] %*% c(param$theta)
```
```{r posterior, message = FALSE, warning = FALSE, echo = FALSE}

hyperparameters <- list(mu = NULL, sigma = NULL,
                          alpha = NULL, beta = NULL,
                        Lambda = NULL)
hyperparameters$mu <- rep(0,p)
hyperparameters$sigma <- diag(1,p,p)
hyperparameters$alpha <- 10
hyperparameters$beta <- 10
hyperparameters$Lambda <- solve(hyperparameters$sigma)
  
# posterior <- target$rpost(nsamp, X, Y, method = "stan",
#                           stan_dir ="../../Stan/normal_horseshoe_noQR.stan",
#                           m0 = 20, scale_intercept = 2.5, chains = 4) # this can take a bit

posterior <- target$rpost(nsamp, X, Y, hyperparameters = hyperparameters, method="conjugate")


cond_mu <- x %*% posterior$theta
cond_mu_full <- X %*% posterior$theta
```
```{r method_in_sample, message=FALSE, warning=FALSE, echo=FALSE}
penalty.factor <- rep(1,p)
force <- NULL
adapt_file <- "gaussian_adapt.RData"
if(!file.exists(adapt_file)){
  theta_selection <- W2L1(X=X, Y = cond_mu_full, 
                          theta=posterior$theta, penalty="selection.lasso",
                          nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
                          infimum.maxit=1e4, maxit = 1e3, gamma = gamma,
                          pseudo_observations = 0, display.progress = TRUE,
                   penalty.factor = penalty.factor, method="selection.variable")
  
  theta_proj <- W2L1(X=X, Y=cond_mu_full, 
                          theta=posterior$theta, penalty="mcp",
                          nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
                          infimum.maxit=1, maxit = 1e3, gamma = gamma,
                          pseudo_observations = pseudo.observations, display.progress = TRUE,
                   penalty.factor = penalty.factor, method="projection")
  theta_SA <- WPSA(X=X, Y=cond_mu_full, 
                        theta=posterior$theta, force = force,
                 p = 2, model.size = c(1:p),
                 iter=10, temps = 10,
                 pseudo.obs = 0,
                 proposal = proposal.fun,
                 options = list(method = c("selection.variable"),
                                energy.distribution = "boltzman",
                                cooling.schedule = "exponential"),
                 display.progress = TRUE
                  )
  theta_SW <- WPSW(X=X, Y=cond_mu_full, 
                          theta=posterior$theta, force = force, p = 2,
                          direction = "backward", 
                          method="selection.variable",
                   display.progress = TRUE
                  )
  save(theta_proj, theta_selection, theta_SA, theta_SW, file=adapt_file)
} else {
  load(adapt_file)
}

```
```{r w2 distances, message=FALSE, warning=FALSE, echo=FALSE}
models <- list(Selection = theta_selection, 
               Projection = theta_proj,
               "Simulated Annealing" = theta_SA,
               "Backward Stepwise" = theta_SW)
gauss_plot_file <- "w2plot.rds"
if ( !file.exists(gauss_plot_file) ) { #only run if file doesn't exist
  w2plot <- plot.compare(models, cond_mu_full, X, posterior$theta, 
                       "w2", c("posterior","mean"), parallel = TRUE) # this can take a LONG time
  w2plots <- w2plot$plot
  w2plot.dat <- w2plot$data
  saveRDS( w2plot.dat, gauss_plot_file )
} else {
  w2plot <- readRDS( gauss_plot_file )
}
```
```{r mse distances, message=FALSE, warning=FALSE, echo=FALSE}
#these should run faster
mseplot_mean <- plot.compare(models, true_mu_full, X, posterior$theta, 
                        "mse", c("mean"), parallel = TRUE)
mseplot_posterior <- plot.compare(models, c(param$theta, rep(0,p-p_star)), X, posterior$theta, 
                        "mse", c("posterior"), parallel = TRUE)


```
## Setup
### Predictors
We can measure variable importance in a variety of settings. Our first example is in a setting with Normally distributed data. We generate $n = `r n`$ predictor variables, $X$, from a Multivariate Normal,
\[ X_i \sim N(0, \Sigma), \] with $\Sigma_{i,i} = 1$ for all $i$ from 1 to $p = `r p-1`.$ The first 5 dimensions of $X$ are correlated, dimensions 6-10 are correlated, dimensions 10-20 are correlated, and dimensions 21 to 30 are correlated with correlations `r target$X$corr`. Thus, $\Sigma$ is block diagonal.

### Parameters
We generate parameters as follows
\[ \beta_{0:5} \sim \text{Unif}(1,2),\]
\[ \beta_{6:10} \sim \text{Unif}(-2,-1), \] 
\[ \beta_{11:20} \sim \text{Unif}(0,0.5), \] 
\[ \beta_{21:30} = 0 ,\] and
\[\sigma^2 = 1.\]

### Outcome
We sample the outcome as
\[Y_i \sim N(\beta_0 + X_i^{\top} \beta_{1:20}, \sigma^2) \]

```{r data_echo, echo=TRUE, eval=FALSE}
<<data>>
```

## Posterior Estimation
We put conjugate priors on our parameters,
\[
  \beta_{0:p} \sim N(0,1)
\] and
\[
  \sigma^2 \sim \text{Inv-Gamma}(10,10),
\]
which facillitates quick estimation via known posterior distributions.
```{r post print, eval=FALSE}
hyperparameters <- list(mu = NULL, sigma = NULL,
                          alpha = NULL, beta = NULL,
                        Lambda = NULL)
hyperparameters$mu <- rep(0,p)
hyperparameters$sigma <- diag(1,p,p)
hyperparameters$alpha <- 10
hyperparameters$beta <- 10
hyperparameters$Lambda <- solve(hyperparameters$sigma)

posterior <- target$rpost(nsamp, X, Y, hyperparameters = hyperparameters, method="conjugate")


cond_mu <- x %*% posterior$theta
cond_mu_full <- X %*% posterior$theta
```


We then utilize our 1) selection variable, 2) location/scale, and 3) projection methods to find coarse posteriors close to our full one. Note: this code may take a bit to run.

```{r adapt print, eval = FALSE}
penalty.factor <- rep(1, p)
force <- NULL

theta_selection <- W2L1(X=X, Y = cond_mu_full, 
                          theta=posterior$theta, penalty="selection.lasso",
                          nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
                          infimum.maxit=1e4, maxit = 1e3, gamma = gamma,
                          pseudo_observations = 0, display.progress = TRUE,
                   penalty.factor = penalty.factor, method="selection.variable")
theta_proj <- W2L1(X=X, Y=cond_mu_full, 
                        theta=posterior$theta, penalty="mcp",
                        nlambda = nlambda, lambda.min.ratio = lambda.min.ratio,
                        infimum.maxit=1, maxit = 1e3, gamma = gamma,
                        pseudo_observations = pseudo.observations, display.progress = TRUE,
                 penalty.factor = penalty.factor, method="projection")
theta_SA <- WPSA(X=X, Y=cond_mu_full, 
                        theta=posterior$theta, force = force,
                 p = 2, model.size = c(1:p),
                 iter=10, temps = 50,
                 pseudo.obs = 0,
                 proposal = proposal.fun,
                 options = list(method = c("selection.variable"),
                                energy.distribution = "boltzman",
                                cooling.schedule = "exponential"),
                 display.progress = TRUE
                  )
  theta_SW <- WPSW(X=X, Y=cond_mu_full, 
                          theta=posterior$theta, force = force, p = 2,
                          direction = "backward", 
                          method="selection.variable",
                   display.progress = TRUE
                  )
```


Then we measure our distance from the posterior predictive mean and posterior distribution.


We also measure the mean-squared error from the true mean, since it is known in this case.


Using the Wasserstein distance, we can see how the coarse versions move closer to both the full posterior. The coarse posteriors also converge in MSE to the true parameters 
```{r dist_plot_post,echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4, fig.width=4, out.width="49%", optipng = '-o7', fig.align = "center", fig.show='hold', fig.cap="2-Wasserstein distance between the full posterior for the regression coefficients and our coarsened version."}
print(w2plot$plot$posterior)
print(mseplot_posterior$plot$posterior)
```

And we can look at the differences the posterior predictive mean and the full posterior predictive mean, as well as the MSE between the coarse posterior and true mean.
```{r dist_plot_mean,echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4, fig.width=4, out.width="49%", optipng = '-o7', fig.align = "center", fig.show='hold', fig.cap="2-Wasserstein distance between coarse and full posterior predictive means and MSE between posterior predictive means and the true means (calculated from the true parameters)"}
print(w2plot$plot$mean)
print(mseplot_mean$plot$mean)
```


Finally, we can see as more covariates are active, the coarse distribution gets close to the truth. Importantly, it looks like the selection method does better here for this individual $i = 512$. However, overall, the projection and location/scale methods do better, which we can see in the above plots.

```{r ridge_plots, ,echo=FALSE, message=FALSE, warning=FALSE, fig.height= 4, fig.width=8, out.width="90%", optipng = '-o7', fig.align = "center"}
select.idx <- 512
means <- lapply(theta, function(tclist) 
              data.frame(value = c(sapply(tclist$theta, function(tc) X[select.idx,] %*% tc)), 
             ncoef = rep(tclist$nzero, each = nsamp)
             ))
names(means) <- c("Selection","Loc./Scale","Projection")

df_means <- as.data.frame(data.table::rbindlist(means))
df_means$Method <- factor(unlist(mapply(function(n,l) {rep(n,each=l)}, n = names(means), l = sapply(means, nrow))))
ridgeplot <- ggplot(df_means, aes(y = factor(ncoef))) + 
  geom_density_ridges(aes(x = value, fill=Method), scale=4, alpha=0.5) + 
  theme_ridges() + ylab("Number of Active Coefficients") + xlab("") +
  scale_fill_jama()
print(ridgeplot)
```

